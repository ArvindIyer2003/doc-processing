{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64aeeef8-274a-4622-b898-3aa0818429a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:7043\n",
      " * Running on http://10.0.0.230:7043\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [12/Jul/2024 10:42:04] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [12/Jul/2024 10:42:04] \"GET /favicon.ico HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [12/Jul/2024 10:42:30] \"POST /upload HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [12/Jul/2024 10:42:30] \"POST /upload HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, url_for, send_file, render_template, jsonify\n",
    "from werkzeug.utils import secure_filename\n",
    "import os\n",
    "import threading\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "import tabula\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import fitz\n",
    "import logging\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "app = Flask(__name__)\n",
    "app.config['UPLOAD_FOLDER'] = 'uploads'\n",
    "app.config['PROCESSED_FOLDER'] = 'processed'\n",
    "app.config['ALLOWED_EXTENSIONS'] = {'pdf'}\n",
    "app.secret_key = 'supersecretkey'\n",
    "\n",
    "for folder in [app.config['UPLOAD_FOLDER'], app.config['PROCESSED_FOLDER']]:\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "def allowed_file(filename):\n",
    "    return '.' in filename and filename.rsplit('.', 1)[1].lower() in app.config['ALLOWED_EXTENSIONS']\n",
    "\n",
    "@app.route('/')\n",
    "def upload_form():\n",
    "    return render_template('upload.html')\n",
    "\n",
    "@app.route('/upload', methods=['POST'])\n",
    "def upload_folder():\n",
    "    folder_type = request.form.get('folder_type')\n",
    "    customer_name = request.form.get('customer_name', '')  # Get customer_name if provided (for OBI)\n",
    "    password = request.form.get('password', '')  # Get the password from the form\n",
    "\n",
    "    files = request.files.getlist('files[]')\n",
    "\n",
    "    if not folder_type or not files:\n",
    "        return jsonify({'message': 'Please select a folder type and upload files.'}), 400\n",
    "\n",
    "    response = {'message': 'Files processed successfully', 'error_files': []}\n",
    "\n",
    "    if folder_type == 'adeo':\n",
    "        processed_files, error_files = process_files(files, process_adeo_file)\n",
    "    elif folder_type == 'obi':\n",
    "        processed_files, error_files = process_files(files, process_obi_file, customer_name=customer_name)\n",
    "    elif folder_type=='CAMS':\n",
    "        processed_files, error_files = process_files(files, CAMS_file, password=password)\n",
    "    elif folder_type=='KFINTECH':\n",
    "        processed_files, error_files = process_files(files, kfintech_file, password=password)\n",
    "    else:\n",
    "        return jsonify({'message': 'Invalid folder type selected.'}), 400\n",
    "\n",
    "    if processed_files:\n",
    "        combined_excel = generate_combined_excel(processed_files, f'{folder_type}_combined_output.xlsx')\n",
    "        response['download_url'] = url_for('download_file', filename=combined_excel)\n",
    "\n",
    "\n",
    "    response['error_files'] = error_files\n",
    "\n",
    "    return jsonify(response)\n",
    "\n",
    "def process_files(files, process_function, **kwargs):\n",
    "    processed_files = []\n",
    "    error_files = []\n",
    "    for file in files:\n",
    "        if allowed_file(file.filename):\n",
    "            filename = secure_filename(file.filename)\n",
    "            file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n",
    "            file.save(file_path)\n",
    "            try:\n",
    "                output_filename = process_function(file_path, **kwargs)\n",
    "                processed_files.append(output_filename)\n",
    "            except Exception as e:\n",
    "                app.logger.error(f\"Error processing file {filename}: {e}\")\n",
    "                error_files.append(filename)\n",
    "        else:\n",
    "            error_files.append(file.filename)\n",
    "    return processed_files, error_files\n",
    "def process_adeo_file(file_path):\n",
    "    # Your existing process_file code renamed to process_adeo_file\n",
    "    try:\n",
    "        text = \"\"\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "\n",
    "        regex_patterns = (\n",
    "            (r'(CO\\d*\\d)', 'PO No'),\n",
    "            (r'(Type : (\\w*\\w))', 'Type'),\n",
    "            (r'(Supplier : (\\d+))', 'Supplier'),\n",
    "            (r'(Validation date : (\\d{2}/\\d{2}/\\d{4}))', 'Validation date'),\n",
    "            (r'(Purchase Incoterm Place : (\\d{2}/\\d{2}/\\d{4}))', 'Purchase Incoterm Place'),\n",
    "            (r'(Ordered for : (\\d+))', 'Ordered for'),\n",
    "            (r'(Amount : (?:(\\d+)(\\s+))?(\\d+(\\.\\d+)?)\\s+([a-zA-Z]+))', 'Amount'),\n",
    "            (r'(Amount : (?:\\d+\\s+)?(\\d+(\\.\\d+)?)\\s+([a-zA-Z]+))', 'Currency'),\n",
    "            (r'(Delivery to : (\\d+))', 'Delivery to'),\n",
    "            (r'(Blanket Order No : (\\w+))', 'Blanket Order No')\n",
    "        )\n",
    "        compiled_patterns = [(re.compile(pattern), label) for pattern, label in regex_patterns]\n",
    "        habi = ''\n",
    "        matches = []\n",
    "        for pattern, label in compiled_patterns:\n",
    "            match = pattern.search(text)\n",
    "            if match:\n",
    "                if label == 'PO No':\n",
    "                    matches.append(match.group(1))\n",
    "                    habi = match.group(1)\n",
    "                elif label == 'Currency':\n",
    "                    matches.append(match.group(4))\n",
    "                elif label == 'Amount':\n",
    "                    if match.group(3):\n",
    "                        thousands = match.group(2)\n",
    "                        hundreds = match.group(4)\n",
    "                        total = thousands + hundreds\n",
    "                        matches.append(total)\n",
    "                    else:\n",
    "                        hundreds = match.group(4)\n",
    "                        matches.append(hundreds)\n",
    "                else:\n",
    "                    matches.append(match.group(2))\n",
    "        if not matches:\n",
    "            raise ValueError(\"No matches found for required fields.\")\n",
    "        matches_tuple = tuple(matches)\n",
    "        d = defaultdict(lambda: '')\n",
    "        list = ['PO No', 'Type', 'Supplier', 'Validation date', 'Purchase Incoterm Place', 'Ordered for', 'Amount', 'Currency', 'Delivery to', 'Blanket Order No']\n",
    "        for i in range(len(matches_tuple)):\n",
    "            d[list[i]] = matches_tuple[i]\n",
    "        output_csv_path = os.path.join(app.config['UPLOAD_FOLDER'], 'tables.csv')\n",
    "        tabula.convert_into(file_path, output_csv_path, output_format=\"csv\", pages='all', stream=True)\n",
    "        df_tables = pd.read_csv(output_csv_path, encoding='latin-1')\n",
    "        if len(df_tables) > 33:\n",
    "            df_subset = df_tables.iloc[33:]\n",
    "            df_subset.drop(df_subset.columns[df_subset.columns.str.contains('unnamed', case=False)], axis=1, inplace=True)\n",
    "            two_row = df_subset.drop(columns=['Supplier Ref.', 'Description', 'GTIN', 'Dim. l*w*h cm', 'Weight Kg', 'Quantity','Amount'])\n",
    "            two_row.rename(columns={\n",
    "                'PCB': 'Quantity',\n",
    "                'Price' : 'Total',\n",
    "                'Nb Master': 'Price',\n",
    "            }, inplace=True)\n",
    "            df_sub = df_tables.iloc[:33]\n",
    "            df_sub.drop(df_sub.columns[df_sub.columns.str.contains('unnamed', case=False)], axis=1, inplace=True)\n",
    "            one_row = df_sub.drop(columns=['Supplier Ref.', 'Description', 'GTIN', 'Dim. l*w*h cm', 'Weight Kg', 'PCB', 'Nb Master'])\n",
    "            one_row.rename(columns = {\n",
    "                'Amount':'Total',\n",
    "            },inplace = True)\n",
    "            data = pd.concat([one_row, two_row], ignore_index=False, sort=False, axis=0)\n",
    "        else:\n",
    "            df_tables.drop(df_tables.columns[df_tables.columns.str.contains('unnamed', case=False)], axis=1, inplace=True)\n",
    "            df_tables.rename(columns = {\n",
    "                'Amount':'Total',\n",
    "            },inplace = True)\n",
    "            data = df_tables.drop(columns=['Supplier Ref.', 'Description', 'GTIN', 'Dim. l*w*h cm', 'Weight Kg', 'PCB', 'Nb Master'])\n",
    "        df_tables = data\n",
    "        df_tables.dropna(inplace=True)\n",
    "        df_tables['Customer Ref.'] = df_tables['Customer Ref.'].astype(int)\n",
    "        df_tables.reset_index(drop=True, inplace=True)\n",
    "        details = [d.copy() for _ in range(len(df_tables))]\n",
    "        df_2 = pd.DataFrame(details)\n",
    "        df_final = pd.concat([df_2, df_tables], ignore_index=False, sort=False, axis=1)\n",
    "        df_final.rename(columns={\n",
    "            'ADEO Key': 'Item_No',\n",
    "            'Ordered for': 'BU',\n",
    "            'Delivery to': 'BU_Delivery',\n",
    "            'Validation date': 'Buyer_PO_Date',\n",
    "            'Purchase Incoterm Place': 'Ship_Date',\n",
    "            'Supplier': 'Supplier_Code',\n",
    "            'Price': 'Unit_Price',\n",
    "            'Quantity': 'Qty'\n",
    "        }, inplace=True)\n",
    "        first_half = df_final[['PO No', 'Supplier_Code', 'BU', 'BU_Delivery', 'Buyer_PO_Date', 'Ship_Date', 'Item_No', 'Unit_Price', 'Qty','Total']]\n",
    "        first_half['Qty'] = first_half['Qty'].astype(str)\n",
    "        first_half['Qty'] = first_half['Qty'].str.replace(r'\\s+(\\d)', r'\\1', regex=True)\n",
    "        first_half['Unit_Price'] = first_half['Unit_Price'].astype(str)\n",
    "        first_half['Unit_Price'] = first_half['Unit_Price'].str.replace(r'\\s+(\\d)', r'\\1', regex=True)\n",
    "        first_half['Total'] = first_half['Total'].astype(str)\n",
    "        first_half['Total'] = first_half['Total'].str.replace(r'\\s+(\\d)', r'\\1', regex=True)\n",
    "        second_half = df_final[['Type', 'Amount', 'Currency', 'Blanket Order No', 'Customer Ref.']]\n",
    "        format = pd.concat([first_half, second_half], ignore_index=False, sort=False, axis=1)\n",
    "        final_output_path = os.path.join(app.config['UPLOAD_FOLDER'], f'{habi}.csv')\n",
    "        format.to_csv(final_output_path, index=False)\n",
    "        return f'{habi}.csv'\n",
    "    except Exception as e:\n",
    "        app.logger.error(f\"Error processing file: {e}\")\n",
    "        raise e\n",
    "\n",
    "def process_obi_file(file_path, customer_name=''):\n",
    "    try:\n",
    "        # Extract text from the PDF using pdfplumber\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            all_text = \"\"\n",
    "            for page in pdf.pages:\n",
    "                text = page.extract_text()\n",
    "                all_text += text + \"\\n\"  # add a newline to separate pages (adjust as needed)\n",
    "        lst = all_text.split('\\n')\n",
    "        hbi=''\n",
    "        pattern_5=re.compile(r'(Passwort:\\w+)')\n",
    "        for line in all_text.split('\\n'):\n",
    "            if pattern_5.match(line):\n",
    "                hbi=line\n",
    "        if lst[0] != 'O R D E R' and hbi:\n",
    "            # Processing for OBI-Lux Tools\n",
    "            # Extracting order number\n",
    "            matches = lst[2].split()[-1]\n",
    "            # Extracting vendor\n",
    "            vendor = lst[3]\n",
    "            s = vendor.split()\n",
    "            if 'Order' in s:\n",
    "                vendor = lst[2]\n",
    "                eg = vendor.split(' Order')\n",
    "                vendor = eg[0]\n",
    "            # Compiling data into a dictionary\n",
    "            from collections import defaultdict\n",
    "            data_dict = defaultdict(str)\n",
    "            data_dict['Po No'] = matches\n",
    "            data_dict['Vendor_Code'] = vendor\n",
    "\n",
    "            # Extracting table data with regex\n",
    "            original_pattern = re.compile(\n",
    "                r'\\d{1}?\\s?'       # match a sequence of digits followed by spaces\n",
    "                r'\\d+\\s+'           # match another sequence of digits followed by spaces\n",
    "                r'\\w+(\\s+\\w+)*\\s*'  # match words with optional additional words separated by spaces, followed by optional spaces\n",
    "            )\n",
    "\n",
    "            combined_pattern = re.compile(\n",
    "                fr'{original_pattern.pattern}'  # include the original pattern as mandatory\n",
    "                r'(?:(?:\\d{4}-\\d{2}-\\d{2}|\\b\\w+\\b|\\s)+)?'  # optionally match dates, words, or spaces\n",
    "                r'\\b\\d{1,4}\\.\\d{2}\\b'  # match a decimal number with 1-4 digits before the decimal point and 2 digits after\n",
    "            )\n",
    "\n",
    "            res = []\n",
    "            for line in all_text.split('\\n'):\n",
    "                if combined_pattern.findall(line):\n",
    "                    res.append(line)\n",
    "            df = []\n",
    "            for i in range(len(res)):\n",
    "                val = res[i].split()\n",
    "                tmp = []\n",
    "                tmp.append(val[1])\n",
    "                for i in range(len(val)-6, len(val)):\n",
    "                    tmp.append(val[i])\n",
    "                df.append(tmp)\n",
    "\n",
    "            # Creating DataFrame\n",
    "            data = pd.DataFrame(df, columns=['Item_No', 'Ship Date', 'Quantity', 'Unit Price', 'Package', 'Price unit Qty', 'Total'])\n",
    "            data = data.drop(columns=['Package'])\n",
    "            data['Unit Price'] = data['Unit Price'].astype(str).str.replace(',', '.')\n",
    "            data['Total'] = data['Total'].astype(str).str.replace('.', '')\n",
    "            data['Total'] = data['Total'].astype(str).str.replace(',', '.')\n",
    "            if customer_name:\n",
    "                data['Customer'] = 'CUST0028'\n",
    "            # Adding additional details\n",
    "            details = [data_dict.copy() for _ in range(len(data))]\n",
    "            details_df = pd.DataFrame(details)\n",
    "            df_final = pd.concat([data, details_df], ignore_index=False, sort=False, axis=1)\n",
    "            # Saving final CSV\n",
    "            final_output_path = os.path.join(app.config['UPLOAD_FOLDER'], f'{matches}.csv')\n",
    "            df_final.to_csv(final_output_path, index=False)\n",
    "            return f'{matches}.csv'\n",
    "        elif lst[0] == 'O R D E R':\n",
    "            regex_patterns = (\n",
    "            (r'(Supplier-No.: (\\d+))', 'Supplier-No'),\n",
    "            (r'(Order-No.: (\\d+))', 'Order-No'),\n",
    "            (r'(Dated: (\\d{2}.\\d{2}.\\d{2}))', 'Date')\n",
    "            )\n",
    "            compiled_patterns = [(re.compile(pattern), label) for pattern, label in regex_patterns]\n",
    "            mat = []\n",
    "            for pattern, label in compiled_patterns:\n",
    "                matc= pattern.search(all_text)\n",
    "                if matc:\n",
    "                    mat.append(matc.group(2))\n",
    "            matches_tuple = tuple(mat)\n",
    "            if not matches_tuple:\n",
    "                raise ValueError(\"No matches found for required fields.\")\n",
    "            from collections import defaultdict\n",
    "            d = defaultdict(lambda: '')\n",
    "            list = ['Supplier-No','Order-No','Date']\n",
    "            for i in range(len(matches_tuple)):\n",
    "                d[list[i]] = matches_tuple[i]\n",
    "            pattern_1=re.compile(r'(\\d{3,}\\s(.*)[A-Z]{3}$)')\n",
    "            pattern_3=re.compile(r'^EUROMATE\\b.*')\n",
    "            matches=[]\n",
    "            ans=''\n",
    "            for line in all_text.split('\\n'):\n",
    "                if pattern_1.match(line):\n",
    "                    matches.append(line)\n",
    "                if pattern_3.match(line):\n",
    "                    ans=line\n",
    "            li=[]\n",
    "            for i in range(len(matches)):\n",
    "                tex=matches[i].split()\n",
    "                li.append(tex[-2])\n",
    "            pattern_2 = r'\\d+(\\.\\d+)?(?=/)'\n",
    "            for i in range(len(li)):\n",
    "                te=li[i]\n",
    "                match = re.search(pattern_2, te)\n",
    "                if match:\n",
    "                    number = match.group()\n",
    "                    li[i]=number\n",
    "            df=[]\n",
    "            for i in range(len(matches)):\n",
    "                val=matches[i].split()\n",
    "                tmp=[]\n",
    "                tmp.append(val[0])\n",
    "                tmp.append(val[-4])\n",
    "                tmp.append(li[i])\n",
    "                df.append(tmp)\n",
    "            data = pd.DataFrame(df,columns=['Item_No','Quantity','Price'])\n",
    "            if customer_name:\n",
    "                if ans:\n",
    "                    data['Customer'] = 'CUST0027'\n",
    "                else:\n",
    "                    data['Customer']='CUST0028'\n",
    "            details = [d.copy() for _ in range(len(data))]\n",
    "            a = pd.DataFrame(details)\n",
    "            df_merged = pd.concat([data,a], ignore_index=False, sort=False,axis=1)\n",
    "            df_merged['Date'] = df_merged['Date'].astype(str).str.replace('.', '-')\n",
    "            import numpy as np\n",
    "            df_merged['Ship-No']=np.nan\n",
    "            final_output_path = os.path.join(app.config['UPLOAD_FOLDER'], f'{matches_tuple[0]}_euromate.csv')\n",
    "            df_merged.to_csv(final_output_path, index=False)\n",
    "            return f'{matches_tuple[0]}_euromate.csv'\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported customer name: {customer_name}\")\n",
    "    except Exception as e:\n",
    "        app.logger.error(f\"Error processing OBI file: {e}\")\n",
    "        raise e\n",
    "def CAMS_file(file_path, password=None):\n",
    "    try:\n",
    "        text = \"\"\n",
    "        with pdfplumber.open(file_path, password=password) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "        start_pattern = re.compile(r'^Folio No:')\n",
    "        end_pattern = re.compile(r'^Closing Unit Balance:')\n",
    "        date_pattern = re.compile(r'\\b\\d{2}-[A-Za-z]{3}-\\d{4}\\b')  # Pattern for DD-MMM-YYYY format\n",
    "        date_pattern_2 = re.compile(r'\\b\\d{2}-[A-Za-z]{3}-\\d{4}\\b\\s+(.*)\\s+\\b\\d{2}-[A-Za-z]{3}-\\d{4}\\b')\n",
    "        folio_pattern = re.compile(r'Folio No:(.*?)(?:PAN:\\s*([A-Za-z]{5}\\d{4}[A-Za-z])|KYC:)')\n",
    "        pan_pattern = re.compile(r'PAN:\\s*([A-Za-z]{5}\\d{4}[A-Za-z])')\n",
    "        within_lines = False\n",
    "        extracted_lines = []\n",
    "        for line in text.split('\\n'):\n",
    "            if start_pattern.match(line):\n",
    "                if extracted_lines and extracted_lines[-1] != \"\":\n",
    "                    extracted_lines.append(\"\")  # Add an empty line as delimiter\n",
    "                within_lines = True\n",
    "            if within_lines and not end_pattern.match(line):\n",
    "                extracted_lines.append(line.strip())\n",
    "            if end_pattern.match(line):\n",
    "                within_lines = False\n",
    "        sections = []\n",
    "        current_section = []\n",
    "        folio_numbers = []\n",
    "        pan_numbers = []\n",
    "        \n",
    "        for line in extracted_lines:\n",
    "            if line == \"\":\n",
    "                if current_section:  # Only append if there are lines in the current section\n",
    "                    sections.append(current_section)\n",
    "                    current_section = []  # Reset current_section for the next section\n",
    "            else:\n",
    "                current_section.append(line)\n",
    "        \n",
    "        # Append the last section if it exists\n",
    "        if current_section:\n",
    "            sections.append(current_section)\n",
    "        filtered_sections = []\n",
    "        for section in sections:\n",
    "            filtered_section = []\n",
    "            folio_number = None\n",
    "            pan_number = None\n",
    "            for line in section:\n",
    "                if folio_pattern.search(line):\n",
    "                    folio_number = folio_pattern.search(line).group(1).strip()\n",
    "                if pan_pattern.search(line):\n",
    "                    pan_number = pan_pattern.search(line).group(1)\n",
    "                if date_pattern.match(line):\n",
    "                    if date_pattern_2.match(line):  # Check again to skip \"date to date\" expressions\n",
    "                        continue\n",
    "                    else:\n",
    "                        filtered_section.append(line)\n",
    "            if filtered_section:\n",
    "                filtered_sections.append((folio_number, pan_number, filtered_section))\n",
    "        df_rows = []\n",
    "\n",
    "    # Extract data and format it for DataFrame\n",
    "        for folio_number, pan_number, section in filtered_sections:\n",
    "            for line in section:\n",
    "                parts = line.split()\n",
    "                date = parts[0]\n",
    "                transaction_parts = []\n",
    "                numeric_parts = []\n",
    "                for part in parts[1:]:\n",
    "                    if re.match(r'^\\([-+]?[0-9]*\\.?[0-9]+(?:,\\d{3})*(?:\\.\\d+)?\\)$', part):  # Check for numbers in parentheses\n",
    "                        numeric_parts.append(part.strip('()'))  # Remove parentheses\n",
    "                        if part.startswith('(') and part.endswith(')'):  # Check if it's a negative number in parentheses\n",
    "                            numeric_parts[-1] = '-' + numeric_parts[-1]  # Add negative sign\n",
    "                    elif re.match(r'^[-+]?[0-9]*\\.?[0-9]+(?:,\\d{3})*(?:\\.\\d+)?$', part):  # Check for regular numbers\n",
    "                        numeric_parts.append(part)\n",
    "                    else:\n",
    "                        transaction_parts.append(part)  # Add non-numeric parts to transaction parts\n",
    "                \n",
    "                transaction = \" \".join(transaction_parts)\n",
    "                \n",
    "                amount = numeric_parts[0] if len(numeric_parts) > 0 else ''\n",
    "                units = numeric_parts[1] if len(numeric_parts) > 1 else ''\n",
    "                nav = numeric_parts[2] if len(numeric_parts) > 2 else ''\n",
    "                unit_balance = numeric_parts[3] if len(numeric_parts) > 3 else ''\n",
    "                \n",
    "                df_rows.append([folio_number, pan_number, date, transaction, amount, units, nav, unit_balance])\n",
    "    \n",
    "        # Create a DataFrame\n",
    "        df = pd.DataFrame(df_rows, columns=['Folio Number', 'PAN Number', 'Date', 'Transaction', 'Amount', 'Units', 'NAV', 'Unit_Balance'])\n",
    "        final_output_path = os.path.join(app.config['UPLOAD_FOLDER'], 'habi.csv')\n",
    "        df.to_csv(final_output_path, index=False)\n",
    "        return 'habi.csv'\n",
    "    except Exception as e:\n",
    "        app.logger.error(f\"Error processing file: {e}\")\n",
    "        raise e\n",
    "\n",
    "def decrypt_and_extract_text(input_pdf_path, password=None):\n",
    "    try:\n",
    "        # Open encrypted PDF file with PyMuPDF\n",
    "        pdf_document = fitz.open(input_pdf_path)\n",
    "\n",
    "        # Check if the PDF is encrypted\n",
    "        if pdf_document.is_encrypted:\n",
    "            # Authenticate and decrypt the PDF\n",
    "            if password and pdf_document.authenticate(password):\n",
    "                # Save the decrypted PDF\n",
    "                decrypted_pdf_path = input_pdf_path.replace('.pdf', '_decrypted.pdf')\n",
    "                pdf_document.save(decrypted_pdf_path)\n",
    "                logging.info(\"File decrypted successfully.\")\n",
    "\n",
    "                # Extract text from decrypted PDF\n",
    "                extracted_text = extract_text_from_pdf(decrypted_pdf_path)\n",
    "                return extracted_text\n",
    "            elif not password:\n",
    "                logging.error(\"Password is required to decrypt the file.\")\n",
    "                raise ValueError(\"Password is required to decrypt the file.\")\n",
    "            else:\n",
    "                logging.error(\"Incorrect password or unable to decrypt.\")\n",
    "                raise ValueError(\"Incorrect password or unable to decrypt.\")\n",
    "        else:\n",
    "            logging.info(\"File is not encrypted.\")\n",
    "\n",
    "            # Extract text directly from the PDF\n",
    "            extracted_text = extract_text_from_pdf(input_pdf_path)\n",
    "            return extracted_text\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing file: {e}\")\n",
    "        raise e\n",
    "    finally:\n",
    "        if 'pdf_document' in locals():\n",
    "            pdf_document.close()\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with fitz.open(pdf_path) as pdf_document:\n",
    "        for page_num in range(len(pdf_document)):\n",
    "            page = pdf_document.load_page(page_num)\n",
    "            text += page.get_text()\n",
    "    return text\n",
    "def kfintech_file(file_path, password=None):\n",
    "    try:\n",
    "        extracted_text = decrypt_and_extract_text(file_path, password=password)\n",
    "        if extracted_text:\n",
    "            pattern = re.compile(\n",
    "                r'PAN: (.*?)(Closing Unit Balance:)',  # Capture PAN number and content between \"PAN:\" and \"Closing balance:\"\n",
    "                re.DOTALL  # Make the dot match newlines\n",
    "            )\n",
    "\n",
    "            matches = pattern.findall(extracted_text)\n",
    "            extracted_content = []\n",
    "            for match in matches:\n",
    "                folio_number, content = match\n",
    "                extracted_content.append(f\"PAN: {folio_number.strip()}\\nContent:\\n{content.strip()}\\n\")\n",
    "\n",
    "            extracted_text_without_function = \"\\n\".join(extracted_content).strip()\n",
    "            text_with_spaces = extracted_text_without_function.replace('\\n', ' ')\n",
    "            processed_text=text_with_spaces.replace(' PAN:', '\\nPAN:')\n",
    "            clean_text = re.sub(r'\\s+NAV.*$', '', processed_text, flags=re.MULTILINE)\n",
    "            date_pattern = r'(\\d{2}-\\w{3}-\\d{4})'\n",
    "            parts = re.split(date_pattern, clean_text)\n",
    "        \n",
    "            # Initialize variables\n",
    "            cleaned_text = parts[0].strip()\n",
    "            previous_date = None\n",
    "            \n",
    "            # Process the text\n",
    "            for i in range(1, len(parts), 2):\n",
    "                current_date = parts[i]\n",
    "                details = parts[i + 1].strip()\n",
    "                # Check if details start with 'PAN:' or the line starts with a date\n",
    "                if (current_date and not cleaned_text.strip().endswith(current_date)):\n",
    "                    cleaned_text += '\\n' + current_date + ' ' + details\n",
    "                else:\n",
    "                    cleaned_text += ' ' + details\n",
    "                \n",
    "                previous_date = current_date\n",
    "            \n",
    "            # Print the cleaned text\n",
    "            clean=cleaned_text.strip()\n",
    "            pattern = r'\\b\\d{2}-[A-Za-z]{3}-\\d{4}\\s+(PAN: [A-Z]{5}[0-9]{4}[A-Z])'\n",
    "            result = re.sub(pattern, r'\\1', clean)\n",
    "            final=result.strip()  # Strip to remove extra whitespace around the text\n",
    "            start_pattern = re.compile(r'^PAN:')\n",
    "            end_pattern = re.compile(r'^Closing Unit Balance:')\n",
    "            date_pattern = re.compile(r'\\b\\d{2}-[A-Za-z]{3}-\\d{4}\\b')  # Pattern for DD-MMM-YYYY format\n",
    "            date_range_pattern = re.compile(r'\\b\\d{2}-[A-Za-z]{3}-\\d{4}\\b\\s+(.*)\\s+\\b\\d{2}-[A-Za-z]{3}-\\d{4}\\b')\n",
    "            folio_pattern = re.compile(r'Folio No :(.*)KYC :')\n",
    "            pan_pattern = re.compile(r'PAN:\\s*([A-Za-z]{5}\\d{4}[A-Za-z])')\n",
    "            \n",
    "            # Flag to track whether we are within the desired lines\n",
    "            within_lines = False\n",
    "            extracted_lines = []\n",
    "            for line in final.split('\\n'):\n",
    "                if start_pattern.match(line):\n",
    "                    if extracted_lines and extracted_lines[-1] != \"\":\n",
    "                        extracted_lines.append(\"\")  # Add an empty line as delimiter\n",
    "                    within_lines = True\n",
    "                if within_lines and not date_range_pattern.search(line):  # Exclude lines matching date range pattern\n",
    "                    extracted_lines.append(line.strip())\n",
    "                if end_pattern.match(line):\n",
    "                    within_lines = False\n",
    "            \n",
    "            # Create a new list of sections based on the delimiter\n",
    "            sections = []\n",
    "            current_section = []\n",
    "            \n",
    "            for line in extracted_lines:\n",
    "                if line == \"\":\n",
    "                    if current_section:  # Only append if there are lines in the current section\n",
    "                        sections.append(current_section)\n",
    "                        current_section = []  # Reset current_section for the next section\n",
    "                else:\n",
    "                    current_section.append(line)\n",
    "            \n",
    "            # Append the last section if it exists\n",
    "            if current_section:\n",
    "                sections.append(current_section)\n",
    "            filtered_sections = []\n",
    "            for section in sections:\n",
    "                filtered_section = []\n",
    "                folio_number = None\n",
    "                pan_number = None\n",
    "                skip_section = False\n",
    "                for line in section:\n",
    "                    if date_range_pattern.search(line):\n",
    "                        skip_section = True\n",
    "                        break\n",
    "                    if folio_pattern.search(line):\n",
    "                        folio_number = folio_pattern.search(line).group(1)\n",
    "                    if pan_pattern.search(line):\n",
    "                        pan_number = pan_pattern.search(line).group(1)\n",
    "                    if date_pattern.match(line):\n",
    "                        if date_range_pattern.match(line):  # Check again to skip \"date to date\" expressions\n",
    "                            continue\n",
    "                        else:\n",
    "                            filtered_section.append(line)\n",
    "                if filtered_section and not skip_section:\n",
    "                    filtered_sections.append((folio_number, pan_number, filtered_section))\n",
    "            date_pattern_n = re.compile(r\"^\\d{2}-[A-Za-z]{3}-\\d{4}$\")\n",
    "            for i in range(len(filtered_sections)):\n",
    "                filtered_sections[i] = (\n",
    "                    filtered_sections[i][0],\n",
    "                    filtered_sections[i][1],\n",
    "                    [entry for entry in filtered_sections[i][2] if not date_pattern_n.match(entry)]\n",
    "                )\n",
    "            df_rows = []\n",
    "            for folio_number, pan_number, section in filtered_sections:\n",
    "                i = 0\n",
    "                while i < len(section):\n",
    "                    line = section[i]\n",
    "                    parts = line.split()\n",
    "                    date = parts[0]\n",
    "                    transaction_parts = []\n",
    "                    numeric_parts = []\n",
    "                    \n",
    "                    for part in parts[1:]:\n",
    "                        if re.match(r'^\\([-+]?(?:\\d{1,3}(?:,\\d{2,3})*|\\d+)(?:\\.\\d+)?\\)$|^\\([-+]?(?:\\d{1,3}(?:,\\d{2,3})*|\\d+)\\.\\d+\\)$', part):  # Check for numbers in parentheses\n",
    "                            numeric_parts.append(part.strip('()'))  # Remove parentheses\n",
    "                            if part.startswith('(') and part.endswith(')'):  # Check if it's a negative number in parentheses\n",
    "                                numeric_parts[-1] = '-' + numeric_parts[-1]  # Add negative sign\n",
    "                        elif re.match(r'^[-+]?(?:\\d{1,3}(?:,\\d{2,3})*|\\d+)(?:\\.\\d+)?$', part):  # Check for regular numbers\n",
    "                            numeric_parts.append(part)\n",
    "                        else:\n",
    "                            transaction_parts.append(part)  # Add non-numeric parts to transaction parts\n",
    "                    \n",
    "                    transaction = \" \".join(transaction_parts)\n",
    "                    \n",
    "                    # Check if transaction contains only \"To\"\n",
    "                    if transaction.strip().lower() == \"to\":\n",
    "                        i += 2  # Skip this line and the next line\n",
    "                        continue\n",
    "                    amount = numeric_parts[0] if len(numeric_parts) > 0 else ''\n",
    "                    units = numeric_parts[1] if len(numeric_parts) > 1 else ''\n",
    "                    nav = numeric_parts[2] if len(numeric_parts) > 2 else ''\n",
    "                    unit_balance = numeric_parts[3] if len(numeric_parts) > 3 else ''\n",
    "                    \n",
    "                    # Append row to df_rows\n",
    "                    df_rows.append([folio_number, pan_number, date, transaction, amount, units, nav, unit_balance])\n",
    "                    \n",
    "                    i += 1\n",
    "    \n",
    "            # Create DataFrame df\n",
    "            df = pd.DataFrame(df_rows, columns=['Folio Number', 'PAN Number', 'Date', 'Transaction', 'Amount', 'Units', 'NAV', 'Unit_Balance'])\n",
    "            final_output_path = os.path.join(app.config['UPLOAD_FOLDER'], 'habi.csv')\n",
    "            df.to_csv(final_output_path, index=False)\n",
    "            return 'habi.csv'\n",
    "    except Exception as e:\n",
    "        app.logger.error(f\"Error processing file: {e}\")\n",
    "        raise e\n",
    "\n",
    "def generate_combined_excel(csv_files, output_filename):\n",
    "    combined_df = pd.concat([pd.read_csv(os.path.join(app.config['UPLOAD_FOLDER'], csv_file)) for csv_file in csv_files], ignore_index=True)\n",
    "    combined_excel_path = os.path.join(app.config['PROCESSED_FOLDER'], output_filename)\n",
    "    combined_df.to_excel(combined_excel_path, index=False)\n",
    "    return output_filename\n",
    "@app.route('/download/<filename>')\n",
    "def download_file(filename):\n",
    "    return send_file(os.path.join(app.config['PROCESSED_FOLDER'], filename), as_attachment=True)\n",
    "    \n",
    "\n",
    "def run_app():\n",
    "    app.run(host='0.0.0.0', port=7043)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    threading.Thread(target=run_app).start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b13a95-873b-47f1-afce-4aee62b60b41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
